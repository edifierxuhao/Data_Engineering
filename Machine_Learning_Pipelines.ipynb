{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to this lesson on ML Pipelines! Here, we'll cover:\n",
    "\n",
    "- Advantages of Machine Learning Pipelines\n",
    "- Scikit-learn Pipeline\n",
    "- Scikit-learn Feature Union\n",
    "- Pipelines and Grid Search\n",
    "- Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corporate Messaging Case Study\n",
    "This corporate message data is from one of the free datasets provided on the [Figure Eight](https://www.figure-eight.com/data-for-everyone/) Platform, licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Next, you'll use NLP to process text data, much like what you'll be doing in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Before we can classify any posts, we'll need to clean and tokenize the text data. Use what you remember from the last lesson on NLP to implement the function `tokenize`. This function should perform the following steps on the string, `text`, using nltk:\n",
    "\n",
    "1. Identify any urls in `text`, and replace each one with the word, `\"urlplaceholder\"`.\n",
    "2. Split `text` into tokens.\n",
    "3. For each token: lemmatize, normalize case, and strip leading and trailing white space.\n",
    "4. Return the tokens in a list!\n",
    "\n",
    "For example, this:\n",
    "```python\n",
    "text = 'Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG'\n",
    "\n",
    "tokenize(text)\n",
    "```\n",
    "should return this:\n",
    "```txt\n",
    "['barclays', 'ceo', 'stress', 'the', 'importance', 'of', 'regulatory', 'and', 'cultural', 'reform', 'in', 'financial', 'service', 'at', 'brussels', 'conference', 'urlplaceholder']\n",
    "```\n",
    "\n",
    "Hint: You'll have to add an import statement to use the `re` package (which supports regular expressions) and two import statements to use the appropriate functions from `nltk`! Add them to this first code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuhao3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/xuhao3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet'])\n",
    "\n",
    "# import statements\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from encodings.aliases import aliases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp855',\n",
       " 'iso8859_15',\n",
       " 'cp1125',\n",
       " 'cp865',\n",
       " 'hp_roman8',\n",
       " 'cp850',\n",
       " 'iso8859_14',\n",
       " 'cp862',\n",
       " 'cp863',\n",
       " 'cp866',\n",
       " 'mac_turkish',\n",
       " 'cp857',\n",
       " 'koi8_r',\n",
       " 'mac_latin2',\n",
       " 'iso8859_16',\n",
       " 'mac_greek',\n",
       " 'latin_1',\n",
       " 'iso8859_13',\n",
       " 'iso8859_2',\n",
       " 'cp852',\n",
       " 'ptcp154',\n",
       " 'cp858',\n",
       " 'iso8859_9',\n",
       " 'iso8859_10',\n",
       " 'mac_roman',\n",
       " 'cp437',\n",
       " 'iso8859_4',\n",
       " 'cp1256',\n",
       " 'iso8859_5',\n",
       " 'cp861',\n",
       " 'cp775',\n",
       " 'cp860',\n",
       " 'cp1251',\n",
       " 'kz1048',\n",
       " 'mac_iceland',\n",
       " 'mac_cyrillic',\n",
       " 'cp864']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "alias_values = set(aliases.values())\n",
    "\n",
    "fit_list = []\n",
    "for i in alias_values:\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv('corporate_messaging.csv', encoding = i)\n",
    "        fit_list.append(i)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "fit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For step 1, the regular expression to detect a url is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    \n",
    "    # replace each url in text string with 'urlplaceholder'\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, 'urlplaceholder')\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG\n",
      "['barclays', 'ceo', 'stress', 'the', 'importance', 'of', 'regulatory', 'and', 'cultural', 'reform', 'in', 'financial', 'service', 'at', 'brussels', 'conference', 'urlplaceholder'] \n",
      "\n",
      "Barclays announces result of Rights Issue http://t.co/LbIqqh3wwG\n",
      "['barclays', 'announces', 'result', 'of', 'rights', 'issue', 'urlplaceholder'] \n",
      "\n",
      "Barclays publishes its prospectus for its å£5.8bn Rights Issue: http://t.co/YZk24iE8G6\n",
      "['barclays', 'publishes', 'it', 'prospectus', 'for', 'it', 'å£5.8bn', 'rights', 'issue', ':', 'urlplaceholder'] \n",
      "\n",
      "Barclays Group Finance Director Chris Lucas is to step down at the end of the week due to ill health http://t.co/nkuHoAfnSD\n",
      "['barclays', 'group', 'finance', 'director', 'chris', 'lucas', 'is', 'to', 'step', 'down', 'at', 'the', 'end', 'of', 'the', 'week', 'due', 'to', 'ill', 'health', 'urlplaceholder'] \n",
      "\n",
      "Barclays announces that Irene McDermott Brown has been appointed as Group Human Resources Director http://t.co/c3fNGY6NMT\n",
      "['barclays', 'announces', 'that', 'irene', 'mcdermott', 'brown', 'ha', 'been', 'appointed', 'a', 'group', 'human', 'resources', 'director', 'urlplaceholder'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test out function\n",
    "X, y = load_data()\n",
    "for message in X[:5]:\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow\n",
    "Complete the steps below to complete the machine learning workflow for this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load data and perform a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y = load_data()\n",
    "\n",
    "# perform train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Nice photo, thanks for giving back! RT @Kentoro47 @ChildrensAidNYC #CitiVolunteers #wagonroadcamp http://t.co/XypKiZk8o2',\n",
       "       'Barclays Wealth Hosts Excellence Awards in Isle of Man http://ow.ly/D7Mr',\n",
       "       'For #WAD2013, remember to take the #ProjectIDesign pledge. Learn more: http://t.co/DeDIgmr8ju',\n",
       "       ...,\n",
       "       '\\x89ÛÏMerck has built a reputation for discovering and developing breakthrough therapeutics.\\x89Û\\x9d \\x89ÛÒ R. Perlmutter #MerckBiz14',\n",
       "       'See how we work with coffee cooperatives to help improve #sustainability of production in Colombia #CSV2011 http://youtu.be/DEy4-vCYPP8',\n",
       "       'RT @atelierstat: Brazilian #Internet users spend 36% of their time on the internet on#SocialNetworks http://t.co/WLnHlHES5T  cc @comScore'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dialogue', 'Information', 'Action', ..., 'Information', 'Action',\n",
       "       'Information'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train classifier\n",
    "* Fit and transform the training data with `CountVectorizer`. Hint: You can include your tokenize function in the `tokenizer` keyword argument!\n",
    "* Fit and transform these word counts with `TfidfTransformer`.\n",
    "* Fit a classifier to these tfidf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate transformers and classifier\n",
    "vect = CountVectorizer(tokenizer=tokenize)\n",
    "tfidf = TfidfTransformer(smooth_idf=False)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Fit and/or transform each to the data\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Predict on test data\n",
    "* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n",
    "* Predict labels on these tfidf values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "# Predict test labels\n",
    "y_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use vect.transform(), not vect.fit_transform()?\n",
    "\n",
    "\n",
    "You do not do a fit_transform on the test data because, when you fit a Random Forest, the Random Forest learns the classification rules based on the values of the features that you provide it. If these rules are to be applied to classify the test set then you need to make sure that the test features are calculated in the same way using the same vocabulary. If the vocabulary of the training and the test features is different, then features will not really make sense as they will reflect a vocabulary that is separate from the one the document was trained on.\n",
    "\n",
    "Now if we specifically talk about CountVectorizer, then consider the following example, let your training data have the following 3 sentences:\n",
    "\n",
    "1. Dog is black.\n",
    "2. Sky is blue.\n",
    "3. Dog is dancing.\n",
    "\n",
    "Now the vocabulary set for this will be {Dog, is, black, sky, blue, dancing}. Now the Random Forest that you will train will try to learn rules based on the count of these 6 vocabulary terms. So your features will be vector of length 6. Now if the test set is as follows:\n",
    "\n",
    "1. Dog is white.\n",
    "2. Sky is black.\n",
    "\n",
    "Now if you use the test data for fit_transform your vocabulary will look like {Dog, white, is, Sky, black}. So here your each document will be represented by a vector of length 5 denoting the counts of each of these terms. Now, this will be like comparing apples with oranges. You learn rules for counts of the previous vocabulary and those rules can not be applied to this vocabulary. This is the reason why you only fit on the training data.\n",
    "\n",
    "Hope that helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Dialogue', 'Dialogue',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Action', 'Dialogue',\n",
       "       'Information', 'Action', 'Action', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Dialogue', 'Information',\n",
       "       'Dialogue', 'Information', 'Information', 'Information',\n",
       "       'Dialogue', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Action', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue', 'Action',\n",
       "       'Information', 'Action', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information', 'Action',\n",
       "       'Information', 'Action', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Dialogue', 'Action', 'Information',\n",
       "       'Action', 'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Dialogue', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Dialogue', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information', 'Dialogue',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Dialogue', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Dialogue', 'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue',\n",
       "       'Information', 'Information', 'Action', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Dialogue', 'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Dialogue', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action', 'Action',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Action', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Dialogue', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Dialogue',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Action', 'Dialogue', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Dialogue', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Information', 'Dialogue',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Action', 'Information', 'Action', 'Information',\n",
       "       'Action', 'Information', 'Information', 'Information', 'Action',\n",
       "       'Information', 'Information', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Action', 'Action',\n",
       "       'Information', 'Action', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Information', 'Action', 'Action', 'Information',\n",
       "       'Information', 'Information', 'Information', 'Information',\n",
       "       'Information', 'Dialogue', 'Information', 'Information',\n",
       "       'Information'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Display results\n",
    "Display a confusion matrix and accuracy score based on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 91   0  33]\n",
      " [  1  37   8]\n",
      " [  5   1 545]]\n",
      "Accuracy: 0.9334257975034674\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = np.unique(y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred, labels = labels)\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Step: Refactor\n",
    "Organize these steps into the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "def display_results(y_pred, y_test):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels = labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =.3,random_state = 42)\n",
    "    \n",
    "    # Instantiate transformers and classifier\n",
    "    vect = CountVectorizer(tokenizer=tokenize)\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # Fit and/or transform each to the data\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Predict test labels\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    \n",
    "    # display results\n",
    "    display_results(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 96   0  28]\n",
      " [  1  37   8]\n",
      " [  5   1 545]]\n",
      "Accuracy: 0.9403606102635229\n"
     ]
    }
   ],
   "source": [
    "# run program\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipeline\n",
    "Below, you'll find a simple example of a machine learning workflow where we generate features from text data using count vectorizer and tf-idf transformer, and then fit it to a random forest classifier. Before we get into using pipelines, let's first use this example to go over some scikit-learn terminology.\n",
    "\n",
    "- **ESTIMATOR**: An estimator is any object that learns from data, whether it's a classification, regression, or clustering algorithm, or a transformer that extracts or filters useful features from raw data. Since estimators learn from data, they each must have a fit method that takes a dataset. In the example below, the CountVectorizer, TfidfTransformer, and RandomForestClassifier are all estimators, and each have a fit method.\n",
    "- **TRANSFORMER**: A transformer is a specific type of estimator that has a fit method to learn from training data, and then a transform method to apply a transformation model to new data. These transformations can include cleaning, reducing, expanding, or generating features. In the example below, CountVectorizer and TfidfTransformer are transformers.\n",
    "- **PREDICTOR**: A predictor is a specific type of estimator that has a predict method to predict on test data based on a supervised learning algorithm, and has a fit method to train the model on training data. The final estimator, RandomForestClassifier, in the example below is a predictor.\n",
    "\n",
    "In machine learning tasks, it's pretty common to have a very specific sequence of transformers to fit to data before applying a final estimator, such as this classifier. And normally, we'd have to initialize all the estimators, `fit` and `transform` the training data for each of the transformers, and then fit to the final estimator. Next, we'd have to call `transform` for each transformer again to the test data, and finally call `predic` on the final estimator.\n",
    "\n",
    "#### Without pipeline:\n",
    "```python\n",
    "    vect = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # train classifier\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "```\n",
    "Fortunately, you can actually automate all of this fitting, transforming, and predicting, by chaining these estimators together into a single estimator object. That single estimator would be scikit-learn's Pipeline. To create this pipeline, we just need a list of (key, value) pairs, where the key is a string containing what you want to name the step, and the value is the estimator object.\n",
    "\n",
    "#### Using pipeline:\n",
    "\n",
    "```python\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier()),\n",
    "    ])\n",
    "\n",
    "    # train classifier\n",
    "    pipeline.fit(Xtrain)\n",
    "\n",
    "    # evaluate all steps on test set\n",
    "    predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "Now, by fitting our pipeline to the training data, we're accomplishing exactly what we would by fitting and transforming each of these steps to our training data one by one. Similarly, when we call `predict` on our pipeline to our test data, we're accomplishing what we would by calling transform on each of our `transformer` objects to our test data and then calling `predict` on our final estimator. Not only does this make our code much shorter and simpler, it has other great advantages, which we'll cover in the next video.\n",
    "\n",
    "Note that every step of this pipeline has to be a transformer, except for the last step, which can be of an estimator type. Pipeline takes on all the methods of whatever the last estimator in its sequence is. For example, here, since the final estimator of our pipeline is a classifier, the pipeline object can be used as a classifier, taking on the `fit` and `predict` methods of its last step. Alternatively, if the last estimator was a transformer, then pipeline would be a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of Using Pipeline\n",
    "Below are two videos explaining the advantages of using scikit-learn's Pipeline as seen in the previous video.\n",
    "\n",
    "1. Simplicity and Convencience\n",
    "\n",
    "\n",
    "- Automates repetitive steps - Chaining all of your steps into one estimator allows you to fit and predict on all steps of your sequence automatically with one call. It handles smaller steps for you, so you can focus on implementing higher level changes swiftly and efficiently.\n",
    "\n",
    "- Easily understandable workflow - Not only does this make your code more concise, it also makes your workflow much easier to understand and modify. Without Pipeline, your model can easily turn into messy spaghetti code from all the adjustments and experimentation required to improve your model.\n",
    "\n",
    "- Reduces mental workload - Because Pipeline automates the intermediate actions required to execute each step, it reduces the mental burden of having to keep track of all your data transformations. Using Pipeline may require some extra work at the beginning of your modeling process, but it prevents a lot of headaches later on.\n",
    "******\n",
    "\n",
    "2. Optimizing Entire Workflow\n",
    "\n",
    "\n",
    "- GRID SEARCH: Method that automates the process of testing different hyper parameters to optimize a model.\n",
    "- By running grid search on your pipeline, you're able to optimize your entire workflow, including data transformation and modeling steps. This accounts for any interactions among the steps that may affect the final metrics.\n",
    "- Without grid search, tuning these parameters can be painfully slow, incomplete, and messy.\n",
    "\n",
    "******\n",
    "\n",
    "3. Preventing Data leakage\n",
    "\n",
    "\n",
    "- Using Pipeline, all transformations for data preparation and feature extractions occur within each fold of the cross validation process.\n",
    "- This prevents common mistakes where you’d allow your training process to be influenced by your test data - for example, if you used the entire training dataset to normalize or extract features from your data.\n",
    "\n",
    "You'll see what we mean by this in a video later in this lesson about using Pipeline with GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Pipeline\n",
    "Using what you learning about pipelining, rewrite your machine learning code from the last section to use sklearn's Pipeline. For reference, the previous main function implementation is provided in the second to last cell. Refactor this in the last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    vect = CountVectorizer(tokenizer=tokenize)\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    # train classifier\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # predict on test data\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "    # display results\n",
    "    display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite the main function to use sklearn's `Pipeline` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier()),\n",
    "    ])\n",
    "        \n",
    "    # train classifier\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # predict on test data\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # display results\n",
    "    display_results(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 93   0  25]\n",
      " [  0  25   7]\n",
      " [  6   2 443]]\n",
      "Accuracy: 0.9334442595673876\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines And Feature Unions\n",
    "- FEATURE UNION: [Feature union](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) is a class in scikit-learn’s Pipeline module that allows us to perform steps in parallel and take the union of their results for the next step.\n",
    "- A pipeline performs a list of steps in a linear sequence, while a feature union performs a list of steps in parallel and then combines their results.\n",
    "- In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions.\n",
    "\n",
    "## Using Feature Union\n",
    "Taking the example from the previous video, let's say you wanted to extract two different kinds of features from the same text column - tfidf values, and the length of the text. Your first approach might be to create an additional column from the text column called `text_length` like this. Then both `text` and `text_length` can be part of your feature matrix. But now your pipeline would break. You can't run `CountVectorizer` on NumPy arrays of strings and integers.\n",
    "\n",
    "```python\n",
    "df['txt_length'] = df['text'].apply(len)\n",
    "X = df[['text', 'txt_length']].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "Let's say you had a custom transformer called `TextLengthExtractor`. Now, you could leave `X_train` as just the original text column, if you could figure out how to add the text length extractor to your pipeline. If only you could fit it on the original text data, rather than the output of the previous transformer. But you need both the outputs of `TfidfTransformer` and `TextLengthExtractor` to feed into the classifier as input.\n",
    "\n",
    "```python\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('txt_length', TextLengthExtractor()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "- Feature unions are super helpful for handling these situations, where we need to run two steps in parallel on the same data and combine their results to pass into the next step.\n",
    "- Like pipelines, feature unions are built using a list of `(key, value)` pairs, where the key is the string that you want to name a step, and the value is the estimator object. Also like pipelines, feature unions combine a list of estimators to become a single estimator. However, a feature union runs its estimators in parallel, rather than in a sequence as a pipeline does. In this example, the estimators run in parallel are `nlp_pipeline` and `text_length`. Notice we use a pipeline in this feature union to make sure the count vectorizer and tfidf transformer steps are still running in sequence.\n",
    "\n",
    "```python\n",
    "X = df['text'].values\n",
    "y = df['label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('nlp_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer())\n",
    "            ('tfidf', TfidfTransformer())\n",
    "             ])),\n",
    "\n",
    "        ('txt_len', TextLengthExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# train classifier\n",
    "pipeline.fit(Xtrain)\n",
    "\n",
    "# predict on test data\n",
    "predicted = pipeline.predict(Xtest)\n",
    "```\n",
    "\n",
    "- Now, our pipeline doesn't break and uses both features! This would be equivalent to this code.\n",
    "\n",
    "\n",
    "```python\n",
    "vect = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "txt_len = TextLengthExtractor()\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# train classifier\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "\n",
    "X_train_len = txt_len.fit_transform(X_train)\n",
    "X_train_features = hstack([X_train_tfidf, X_train_len])\n",
    "clf.fit(X_train_features, y_train)\n",
    "\n",
    "# predict on test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "X_test_len = txt_len.transform(X_test)\n",
    "X_test_features = hstack([X_test_tfidf, X_test_len])\n",
    "y_pred = clf.predict(X_test_features)\n",
    "```\n",
    "\n",
    "- The tfidf transformer and the text length extractor are fit to the input data, in this case the raw data, independently. They are then performed in parallel, and their outputs are combined and passed to the next estimator, in this case, the classifier.\n",
    "\n",
    "Read more about feature unions in Scikit-learn's [user guide](https://scikit-learn.org/stable/modules/pipeline.html#feature-union)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Feature Union\n",
    "Using the given custom transformer, `StartingVerbExtractor`, add a feature union to your pipeline to incorporate a feature that indicates with a boolean value whether the starting token of a post is identified as a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your pipeline to have this structure:\n",
    "- Pipeline\n",
    "    - feature union\n",
    "        - text pipeline\n",
    "            - count vectorizer\n",
    "            - TFIDF transformer\n",
    "        - starting verb extractor\n",
    "    - classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "       ('features', FeatureUnion([\n",
    "           \n",
    "           ('nlp_pipline', Pipeline([\n",
    "               ('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer())\n",
    "               ])),\n",
    "           \n",
    "           ('starting_verb', StartingVerbExtractor())           \n",
    "        ])),\n",
    "       \n",
    "        ('clf', RandomForestClassifier())\n",
    "        \n",
    "    ])\n",
    "\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run program to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 97   0  15]\n",
      " [  2  21  10]\n",
      " [  6   0 450]]\n",
      "Accuracy: 0.9450915141430949\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = model_pipeline()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    display_results(y_test, y_pred)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Transformer\n",
    "In the last section, you used a custom transformer that extracted whether each text started with a verb. You can implement a custom transformer yourself by extending the base class in Scikit-Learn. Let's take a look at a a very simple example that multiplies the input data by ten.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TenMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X * 10\n",
    "```\n",
    "\n",
    "Remember, all estimators have a fit method, and since this is a transformer, it also has a transform method.\n",
    "\n",
    "- **FIT METHOD**: This takes in a 2d array `X` for the feature data and a 1d array `y` for the target labels. Inside the `fit` method, we simply return `self`. This allows us to chain methods together, since the result on calling fit on the transformer is still the transformer object. This method is required to be compatible with scikit-learn.\n",
    "- **TRANSFORM METHOD**: The transform function is where we include the code that well, transforms the data. In this case, we return the data in X multiplied by 10. This `transform` method also takes a 2d array `X`.\n",
    "\n",
    "Let's test our new transformer, by entering the code below in the interactive python interpreter in the terminal, ipython. We can also do this in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TenMultiplier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60, 30, 70, 40, 70])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplier = TenMultiplier()\n",
    "\n",
    "X = np.array([6, 3, 7, 4, 7])\n",
    "multiplier.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Next, we'll create a custom transformer that has a bit more significance. Let's build a case normalizer, which simply converts all text to lowercase. We aren't setting anything in our init method, so we can actually remove that. We can leave our fit method as is, and focus on the transform method. We can lowercase all the values in X by applying a lambda function that calls lower on each value. We'll have to wrap this in a pandas Series to be able to use this apply function.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CaseNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.Series(X).apply(lambda x: x.lower()).values\n",
    "\n",
    "case_normalizer = CaseNormalizer()\n",
    "\n",
    "X = np.array(['Implementing', 'a', 'Custom', 'Transformer', 'from', 'SCIKIT-LEARN'])\n",
    "case_normalizer.transform(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['implementing', 'a', 'custom', 'transformer', 'from',\n",
       "       'scikit-learn'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CaseNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.Series(X).apply(lambda x: x.lower()).values\n",
    "\n",
    "case_normalizer = CaseNormalizer()\n",
    "\n",
    "X = np.array(['Implementing', 'a', 'Custom', 'Transformer', 'from', 'SCIKIT-LEARN'])\n",
    "case_normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! It's a good idea to learn how to write your own custom functions - it allows you to have more control and flexibility with your machine learning pipelines.\n",
    "\n",
    "Another way to create custom transformers is by using this FunctionTransformer from scikit-learn's preprocessing module. This allows you to wrap an existing function to become a transformer. This provides less flexibility, but is much simpler. You can learn more about this in the link below.\n",
    "\n",
    "Read more about using FunctionTransformer to create custom transformers [here](http://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer).\n",
    "\n",
    "\n",
    "Often, you will want to convert an existing Python function into a transformer to assist in data cleaning or processing. You can implement a transformer from an arbitrary function with FunctionTransformer. For example, to build a transformer that applies a log transformation in a pipeline, do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.69314718],\n",
       "       [1.09861229, 1.38629436]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the StartingVerbExtractor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        # tokenize by sentences\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            # tokenize each sentence into words and tag part of speech\n",
    "            pos_tags = nltk.pos_tag(sentence)\n",
    "\n",
    "            # index pos_tags to get the first word and part of speech tag\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            \n",
    "            # return true if the first word is an appropriate verb or RT for retweet\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "\n",
    "            return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # apply starting_verb function to all values in X\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb).values # pay attention to self.starting_verb\n",
    "\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 78   0  36]\n",
      " [  0  22   8]\n",
      " [  3   1 453]]\n",
      "Accuracy: 0.9201331114808652\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def model_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = model_pipeline()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    display_results(y_test, y_pred)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines And Grid Search\n",
    "As mentioned earlier in the lesson, a powerful benefit to using pipeline is the ability to perform a grid search on your entire workflow.\n",
    "\n",
    "Most machine learning algorithms have a set of parameters that need tuning. Grid search is a tool that allows you to define a “grid” of parameters, or a set of values to check. Your computer automates the process of trying out all possible combinations of values. **Grid search scores each combination with cross validation, and uses the cross validation score to determine the parameters that produce the most optimal model**.\n",
    "\n",
    "Running grid search on your pipeline allows you to try many parameter values thoroughly and conveniently, for both your data transformations and estimators.\n",
    "\n",
    "And again, although you can also run grid search on just a single classifier, running it on your whole pipeline helps you test multiple parameter combinations across your entire pipeline. This accounts for interactions among parameters not just in your model, but data preparation steps as well.\n",
    "\n",
    "Let’s see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipeline with GridSearchCV\n",
    "As you may have seen before, grid search can be used to optimize hyper parameters of a model. Here is a simple example that uses grid search to find parameters for a support vector classifier. All you need to do is create a dictionary of parameters to search, using keys for the names of the parameters and values for the list of parameter values to check. Then, pass the model and parameter grid to the grid search object. Now when you call fit on this grid search object, it will run cross validation on all different combinations of these parameters to find the best combination of parameters for the model.\n",
    "\n",
    "```python\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C':[1, 10]\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now consider if we had a data preprocessing step, where we standardized the data using StandardScaler like this.\n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(X_train)\n",
    "\n",
    "parameters = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C':[1, 10]\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(scaled_data, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may seem okay at first, but if you standardize your whole training dataset, and then use cross validation in grid search to evaluate your model, you've got **data leakage**. Let me explain. Grid search uses cross validation to score your model, meaning it splits your training data into folds of train and validation sets, trains your model on the train set, and scores it on the validation set, and does this multiple times.\n",
    "\n",
    "However, each time, or fold, that this happens, the model already has knowledge of the validation set because all the data was rescaled based on the distribution of the whole training dataset. Important factors like the mean and standard deviation are influenced by the whole dataset. This means the model perform better than it really should on unseen data, since information about the validation set is always baked into the rescaled values of your train dataset.\n",
    "\n",
    "The way to fix this, would be to make sure you run standard scaler only on the training set, and not the validation set within each fold of cross validation. Pipelines allow you to do just this.\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', SVC())\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'scaler__with_mean': [True, False]\n",
    "    'clf__kernel': ['linear', 'rbf'],\n",
    "    'clf__C':[1, 10]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "```\n",
    "\n",
    "Now, since the rescaling is included as part of the pipeline, the standardization doesn't happen until we run grid search. Meaning in each fold of cross validation, the rescaling is done only on the data that the model is trained on, preventing leakage from the validation set. As you can see, pipelines are very valuable to removing the risk of data leakage during the data preparation process.\n",
    "\n",
    "### Note on Run Time\n",
    "Running grid search can take a while, especially if you are searching over a lot of parameters! If you want to reduce it to a few minutes, try commenting out some of your parameters to grid search over just 1 or 2 parameters with a small number of values each. Once you know that works, feel free to add more parameters and see how well your final model can perform! You can try this out in the next page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Let's incorporate grid search into your modeling process. To start, include an import statement for `GridSearchCV` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View parameters in pipeline\n",
    "Before modifying your build_model method to include grid search, view the parameters in your pipeline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "\n",
    "        ('text_pipeline', Pipeline([\n",
    "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])),\n",
    "\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('features', FeatureUnion(n_jobs=None,\n",
       "                transformer_list=[('text_pipeline',\n",
       "                                   Pipeline(memory=None,\n",
       "                                            steps=[('vect',\n",
       "                                                    CountVectorizer(analyzer='word',\n",
       "                                                                    binary=False,\n",
       "                                                                    decode_error='strict',\n",
       "                                                                    dtype=<class 'numpy.int64'>,\n",
       "                                                                    encoding='utf-8',\n",
       "                                                                    input='content',\n",
       "                                                                    lowercase=True,\n",
       "                                                                    max_df=1.0,\n",
       "                                                                    max_features=None,\n",
       "                                                                    min_df=1,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 1),\n",
       "                                                                    preprocessor=None,\n",
       "                                                                    stop_words=None,\n",
       "                                                                    strip_accents=None,\n",
       "                                                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                    tokenizer=<function tokenize at 0x1a23bd0200>,\n",
       "                                                                    vocabulary=None)),\n",
       "                                                   ('tfidf',\n",
       "                                                    TfidfTransformer(norm='l2',\n",
       "                                                                     smooth_idf=True,\n",
       "                                                                     sublinear_tf=False,\n",
       "                                                                     use_idf=True))],\n",
       "                                            verbose=False)),\n",
       "                                  ('starting_verb', StartingVerbExtractor())],\n",
       "                transformer_weights=None, verbose=False)),\n",
       "  ('clf',\n",
       "   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False))],\n",
       " 'verbose': False,\n",
       " 'features': FeatureUnion(n_jobs=None,\n",
       "              transformer_list=[('text_pipeline',\n",
       "                                 Pipeline(memory=None,\n",
       "                                          steps=[('vect',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_df=1,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               1),\n",
       "                                                                  preprocessor=None,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                  tokenizer=<function tokenize at 0x1a23bd0200>,\n",
       "                                                                  vocabulary=None)),\n",
       "                                                 ('tfidf',\n",
       "                                                  TfidfTransformer(norm='l2',\n",
       "                                                                   smooth_idf=True,\n",
       "                                                                   sublinear_tf=False,\n",
       "                                                                   use_idf=True))],\n",
       "                                          verbose=False)),\n",
       "                                ('starting_verb', StartingVerbExtractor())],\n",
       "              transformer_weights=None, verbose=False),\n",
       " 'clf': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'features__n_jobs': None,\n",
       " 'features__transformer_list': [('text_pipeline',\n",
       "   Pipeline(memory=None,\n",
       "            steps=[('vect',\n",
       "                    CountVectorizer(analyzer='word', binary=False,\n",
       "                                    decode_error='strict',\n",
       "                                    dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                    input='content', lowercase=True, max_df=1.0,\n",
       "                                    max_features=None, min_df=1,\n",
       "                                    ngram_range=(1, 1), preprocessor=None,\n",
       "                                    stop_words=None, strip_accents=None,\n",
       "                                    token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                    tokenizer=<function tokenize at 0x1a23bd0200>,\n",
       "                                    vocabulary=None)),\n",
       "                   ('tfidf',\n",
       "                    TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                     sublinear_tf=False, use_idf=True))],\n",
       "            verbose=False)),\n",
       "  ('starting_verb', StartingVerbExtractor())],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__verbose': False,\n",
       " 'features__text_pipeline': Pipeline(memory=None,\n",
       "          steps=[('vect',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=<function tokenize at 0x1a23bd0200>,\n",
       "                                  vocabulary=None)),\n",
       "                 ('tfidf',\n",
       "                  TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                   sublinear_tf=False, use_idf=True))],\n",
       "          verbose=False),\n",
       " 'features__starting_verb': StartingVerbExtractor(),\n",
       " 'features__text_pipeline__memory': None,\n",
       " 'features__text_pipeline__steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=<function tokenize at 0x1a23bd0200>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))],\n",
       " 'features__text_pipeline__verbose': False,\n",
       " 'features__text_pipeline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=<function tokenize at 0x1a23bd0200>, vocabulary=None),\n",
       " 'features__text_pipeline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'features__text_pipeline__vect__analyzer': 'word',\n",
       " 'features__text_pipeline__vect__binary': False,\n",
       " 'features__text_pipeline__vect__decode_error': 'strict',\n",
       " 'features__text_pipeline__vect__dtype': numpy.int64,\n",
       " 'features__text_pipeline__vect__encoding': 'utf-8',\n",
       " 'features__text_pipeline__vect__input': 'content',\n",
       " 'features__text_pipeline__vect__lowercase': True,\n",
       " 'features__text_pipeline__vect__max_df': 1.0,\n",
       " 'features__text_pipeline__vect__max_features': None,\n",
       " 'features__text_pipeline__vect__min_df': 1,\n",
       " 'features__text_pipeline__vect__ngram_range': (1, 1),\n",
       " 'features__text_pipeline__vect__preprocessor': None,\n",
       " 'features__text_pipeline__vect__stop_words': None,\n",
       " 'features__text_pipeline__vect__strip_accents': None,\n",
       " 'features__text_pipeline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text_pipeline__vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'features__text_pipeline__vect__vocabulary': None,\n",
       " 'features__text_pipeline__tfidf__norm': 'l2',\n",
       " 'features__text_pipeline__tfidf__smooth_idf': True,\n",
       " 'features__text_pipeline__tfidf__sublinear_tf': False,\n",
       " 'features__text_pipeline__tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__ccp_alpha': 0.0,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__max_samples': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_impurity_split': None,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify your `build_model` function to return a GridSearchCV object.\n",
    "Try to grid search some parameters in your data transformation steps as well as those for your classifier! Browse the parameters you can search above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "    \n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = { \n",
    "        'features__text_pipeline__vect__max_df': (0.5, 1.0),\n",
    "        'clf__n_estimators':[100,1000],\n",
    "        'clf__min_samples_leaf': [1,10]\n",
    "\n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run program to test\n",
    "Running grid search can take a while, especially if you are searching over a lot of parameters! If you want to reduce it to a few minutes, try commenting out some of your parameters to grid search over just 1 or 2 parameters with a small number of values each. Once you know that works, feel free to add more parameters and see how well your final model can perform! You can try this out in the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 92   0  17]\n",
      " [  3  26   4]\n",
      " [  6   0 453]]\n",
      "Accuracy: 0.9500831946755408\n",
      "\n",
      "Best Parameters: {'clf__min_samples_leaf': 1, 'clf__n_estimators': 1000, 'features__text_pipeline__vect__max_df': 0.5}\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def display_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
